{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) - 899118\n",
      "len(val_data) - 99902\n",
      "18.668502 M parameters\n",
      "step 0: train loss 10.2289, val loss 10.2324, Elapsed time: 0 minutes, 33.818332 seconds\n",
      "step 500: train loss 5.1298, val loss 5.0903, Elapsed time: 2 minutes, 44.161362 seconds\n",
      "step 1000: train loss 4.5443, val loss 4.5441, Elapsed time: 5 minutes, 0.244241 seconds\n",
      "step 1500: train loss 4.2544, val loss 4.2801, Elapsed time: 7 minutes, 30.729283 seconds\n",
      "step 2000: train loss 3.9924, val loss 4.0548, Elapsed time: 12 minutes, 55.439457 seconds\n",
      "step 2500: train loss 3.7844, val loss 3.9121, Elapsed time: 17 minutes, 49.580814 seconds\n",
      "step 3000: train loss 3.5909, val loss 3.7764, Elapsed time: 22 minutes, 58.019260 seconds\n",
      "step 3500: train loss 3.4492, val loss 3.6787, Elapsed time: 26 minutes, 41.843974 seconds\n",
      "step 4000: train loss 3.3097, val loss 3.6088, Elapsed time: 32 minutes, 26.344057 seconds\n",
      "step 4500: train loss 3.1876, val loss 3.5586, Elapsed time: 35 minutes, 24.510722 seconds\n",
      "step 5000: train loss 3.0776, val loss 3.5077, Elapsed time: 40 minutes, 35.132730 seconds\n",
      "step 5500: train loss 2.9754, val loss 3.4818, Elapsed time: 46 minutes, 7.150277 seconds\n",
      "step 6000: train loss 2.8831, val loss 3.4455, Elapsed time: 51 minutes, 27.262511 seconds\n",
      "step 6500: train loss 2.7848, val loss 3.4347, Elapsed time: 56 minutes, 15.338750 seconds\n",
      "step 7000: train loss 2.7027, val loss 3.4133, Elapsed time: 61 minutes, 5.218116 seconds\n",
      "step 7500: train loss 2.6176, val loss 3.4106, Elapsed time: 66 minutes, 44.061810 seconds\n",
      "step 8000: train loss 2.5381, val loss 3.4008, Elapsed time: 72 minutes, 45.042202 seconds\n",
      "step 8500: train loss 2.4637, val loss 3.4066, Elapsed time: 78 minutes, 14.952632 seconds\n",
      "step 9000: train loss 2.3800, val loss 3.3982, Elapsed time: 83 minutes, 11.706887 seconds\n",
      "step 9500: train loss 2.3118, val loss 3.4207, Elapsed time: 88 minutes, 16.612487 seconds\n",
      "step 10000: train loss 2.2459, val loss 3.4208, Elapsed time: 93 minutes, 24.982741 seconds\n",
      "step 10500: train loss 2.1827, val loss 3.4212, Elapsed time: 98 minutes, 36.899117 seconds\n",
      "step 11000: train loss 2.1184, val loss 3.4319, Elapsed time: 103 minutes, 56.444336 seconds\n",
      "step 11500: train loss 2.0628, val loss 3.4530, Elapsed time: 109 minutes, 21.541885 seconds\n",
      "step 12000: train loss 2.0008, val loss 3.4654, Elapsed time: 114 minutes, 52.393196 seconds\n",
      "step 12500: train loss 1.9488, val loss 3.4779, Elapsed time: 120 minutes, 26.826793 seconds\n",
      "step 13000: train loss 1.9008, val loss 3.4943, Elapsed time: 126 minutes, 16.149237 seconds\n",
      "step 13500: train loss 1.8518, val loss 3.5072, Elapsed time: 131 minutes, 51.561402 seconds\n",
      "step 14000: train loss 1.7952, val loss 3.5436, Elapsed time: 137 minutes, 46.179662 seconds\n",
      "step 14500: train loss 1.7527, val loss 3.5679, Elapsed time: 142 minutes, 56.768848 seconds\n",
      "step 15000: train loss 1.7087, val loss 3.5694, Elapsed time: 148 minutes, 31.587797 seconds\n",
      "step 15500: train loss 1.6672, val loss 3.5968, Elapsed time: 154 minutes, 20.759591 seconds\n",
      "step 16000: train loss 1.6219, val loss 3.6329, Elapsed time: 159 minutes, 58.249831 seconds\n",
      "step 16500: train loss 1.5835, val loss 3.6545, Elapsed time: 165 minutes, 32.081806 seconds\n",
      "step 17000: train loss 1.5442, val loss 3.6830, Elapsed time: 170 minutes, 1.136242 seconds\n",
      "step 17500: train loss 1.5019, val loss 3.6922, Elapsed time: 174 minutes, 11.356711 seconds\n",
      "step 18000: train loss 1.4653, val loss 3.7200, Elapsed time: 179 minutes, 22.171253 seconds\n",
      "step 18500: train loss 1.4260, val loss 3.7434, Elapsed time: 185 minutes, 1.395694 seconds\n",
      "step 19000: train loss 1.3948, val loss 3.7581, Elapsed time: 190 minutes, 33.054256 seconds\n",
      "step 19500: train loss 1.3604, val loss 3.7939, Elapsed time: 195 minutes, 59.057540 seconds\n",
      "step 20000: train loss 1.3254, val loss 3.8281, Elapsed time: 201 minutes, 11.181099 seconds\n",
      "step 20500: train loss 1.3000, val loss 3.8424, Elapsed time: 205 minutes, 54.051332 seconds\n",
      "step 21000: train loss 1.2654, val loss 3.8937, Elapsed time: 211 minutes, 6.143573 seconds\n",
      "step 21500: train loss 1.2328, val loss 3.9002, Elapsed time: 215 minutes, 27.350803 seconds\n",
      "step 22000: train loss 1.2066, val loss 3.9182, Elapsed time: 220 minutes, 56.426804 seconds\n",
      "step 22500: train loss 1.1793, val loss 3.9580, Elapsed time: 225 minutes, 59.671292 seconds\n",
      "step 23000: train loss 1.1498, val loss 3.9683, Elapsed time: 230 minutes, 27.680092 seconds\n",
      "step 23500: train loss 1.1252, val loss 4.0206, Elapsed time: 235 minutes, 40.286948 seconds\n",
      "step 24000: train loss 1.0980, val loss 4.0406, Elapsed time: 239 minutes, 57.786596 seconds\n",
      "step 24500: train loss 1.0720, val loss 4.0677, Elapsed time: 245 minutes, 4.118490 seconds\n",
      "step 24999: train loss 1.0477, val loss 4.0864, Elapsed time: 250 minutes, 45.237972 seconds\n",
      "\" Lila and Ben smile at each other. They are different. They say, \"Sara, Ben, you are strong is mine!\" Ben says. He runs to his house bags to Sam. hugs him. \"Come on, let's leave the house. I have a picnic!\" But we have a bath.\" I have a surprise for you. You can play with. I have some crayons and some blocks and some cups. But you have one pair of them asleep. It is wet and yucky,\" Ben and a small book. He says, \"My name is Ben. I have a drawing. I have many blocks. Can I\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import timeit\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 25000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the text data\n",
    "with open('tiny_story.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a mapping from words to integers\n",
    "stoi = {word: i for i, word in enumerate(vocab)}\n",
    "itos = {i: word for i, word in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[word] for word in s.split()]\n",
    "decode = lambda l: ' '.join([itos[i] for i in l])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"len(train_data) - {len(train_data)}\")\n",
    "print(f\"len(val_data) - {len(val_data)}\")\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class WordLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = WordLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, Elapsed time: {int(minutes)} minutes, {seconds:.6f} seconds\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a huge mouse who lived in the tree. It was the sun, the most interesting things the all day ever! One day the puddle was rough but no matter how hard it almost touched it with its huge wings. When the leaves stopped, the ground was rough to a bird. The bird said \"Come on! Let's go and take care of it.\" So the ship approached the tree said and ventured back home. The puddle was very happy. It was safe and happy. The bird thanked it for its warning. Then it saw its special friend of the tree and smiled. Story end Story Start Once upon a time, there was a fat cat. The cat loved to go to a wide open store. The cat was tall and worked all day long. The cat liked to look for all the colors. At the end of the local kids saw a big box. They decided to explore the house and open it. The bird was gentle, so happy to Fin's music. But the cat didn't listen to them. So, the dog decided to wait. He danced and bounced joyfully even faster. They saw more and trees and trees. The cat was happy to meet the copper place to bounce around! They practiced every day, and soon, they would go to catch each other until one day, when they woke up, they were both very nervous. They said it was time to go home. So, they went to the same tunnel more size and shouted with joy. Now, they left the cat on the island. Tom and Tina stayed friends with the high tree for a long time. They watched the whole movie and fly away until it went away. They were happy and excited to go back to the park. Story end Story Start Once upon a time, there was a panda who was very hairy. He had lots of friends, but he was also excited to pick some strawberries. He had a very clever kite. But, and he was very happy. He took a carrot out of his neighbor's house, and gave fruits to him. With lots of determination, he spread his wings and fly away into the air. But soon the lonely circle was too strong and he dropped free. The lion felt so sad. He kept flying higher and higher. Then he lay there, feeling much better. The leaf tasted so hard in his big and sweet! He kept swimming away and laughing. Then, he remembered the branch was never seen again. Story end Story Start Once there was an angel who was gifted girl named Mary. Mary was very curious about what she wanted to have adventures. One day, Mary saw a pale old white skin in the fireplace. She was scared but wondered what was happening. She ran to the edge of it and walked out. Suddenly, a wise old man came and snatched the whistle from the gently. He said to the\n"
     ]
    }
   ],
   "source": [
    "starting_text = encode(\"Once upon a time\")\n",
    "starting_context = torch.tensor([starting_text], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(starting_context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'generated_model_25Kiters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordLanguageModel()\n",
    "model.load_state_dict(torch.load('generated_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
